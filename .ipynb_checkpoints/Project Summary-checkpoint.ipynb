{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af46ba90",
   "metadata": {},
   "source": [
    "# Project Summary\n",
    "\n",
    "Many machine learning algorithms are not interpretable. Hence the objective of this project is to create an interpretable algorithm which gives accuracy as good as the accuracy of other complex algorithms.\n",
    "\n",
    "To achieve this, the algorithm mCLESS is developed using the least squares method.\n",
    "\n",
    "Experiments are  done on four data sets\n",
    "1. Synthetic Data 1\n",
    "2. Synthetic Data 2\n",
    "3. Iris data\n",
    "4. Wine data\n",
    "\n",
    "Performances of following classifiers are then compared to the performance of mCLESS.\n",
    "1. Logistic Regression\n",
    "2. K-neighboursClassifiers\n",
    "3. SVM - rbf\n",
    "4. RandomForestClassifier\n",
    "\n",
    "The performance meteric used for the experiments is percentage accuracy.\n",
    "\n",
    "All Algorithms are run 100 times, creating random 70:30 split of the data for training and testing. Then the average accuracy is reported for each algorithm. From these experiments , we found that that mCLESS gives accuracy as good as other classifiers at a much less computational cost.\n",
    "\n",
    "## Feature expansion - \n",
    "\n",
    "Feature expansion is done by adding a dimension equal to distance of a data sample from a particular point p in the dataset.\n",
    "\n",
    "The startegies implemented to choose p are as follows:\n",
    "1. Visually choose the point p for synthetic data1 and synthetic data2.\n",
    "2. Chose the center of the the dataset as point p.\n",
    "3. Linesearch for point p along the least sqaures line.\n",
    "4. Linesearch along the triangle formed by the centers of the cluster.\n",
    "    \n",
    "The center of the dataset can be chosen as p because when we add the dimension, all points would go away from center , thus increasing the spaces between the clusters. It can make classses more separable.\n",
    "\n",
    "For Synthetic data 1, the exepriments were done by chosing p as the center of the data , by linesearch and by linesearch of the triangle. We found that accuracy for this dataset remains the same through the experiments.\n",
    "\n",
    "For Synthetic data 2, the data clusters are linearly distributed. If the point p is chosen as the center of the class 1 cluster then the distances of the points in other clusters from p are comparatively larger than the distances of points in cluster of class 1. Thus when we add a dimension with respect to this point, the other clusters raise higher than the class 1 cluster in the higher dimension. This breaks the linearity of the data and it becomes more separable.\n",
    "\n",
    "And from the experiments it is found that when the chosen point p is the center of the cluster of class 1, the accuracy increased from 71.04% to 94.52% by adding one dimension to the dataset. Later from the linesearch the accuracy could be improved just a little bit to 94.85% with point p as [4.4,-0.07].\n",
    "\n",
    "For Iris data,by using p as the center of the data set , the accuracy improved from 82.96% to 95.41%. For doing linesearch for Iris data , we chose the feature at the index 2 and 3 because the classes looked more separable in the pairplot of features at 2 and 3.The accuracy can be futher improved to 97.43% by finding the point p[1.0,0.04] through linesearch.\n",
    "\n",
    "For Wine dataset, the accuracy before adding dimension is 98.61%.The accuracy of wine data did not improve much by trying different strategies to find p.\n",
    "\n",
    "## Conclusion -\n",
    "\n",
    "I have successfully implemented and tested the algorithm mCLESS for multi-class classification, which is also interpretable. The parametres of the classifier are visualized and it showcases why the classifier is interpretable. The weight matrix calculated by the least squares method represents the family of lines for each class such that if the distance of a point from the line $L_j=0$ is maximum, then the point belongs to class $j$.\n",
    "\n",
    "For the datasets which cannot be separated using one versus all techniques, it appears that the drop is accuracy is because the decision boundaries (hyperplanes) become almost parallel and collinear to one another. This project shows that a significant improvement in the prediction accuracy can be made by adding an additional dimension to the data by the equation: $||\\mathbf{x}-\\mathbf{p}||$, where the point $\\mathbf{p}$ is chosen such that it breaks collinearity and facilitates clear separation of the classes. Making the data more separable increases the accuracy of any classification algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03c3f40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add explanation for chosing center of cluster vs center of overall data for data 1\n",
    "# why chose center of chuster class 1 as p for data 2\n",
    "# how p was chosen for higher dim data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca354741",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
